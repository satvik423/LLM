{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory\n",
    "ConversationBufferMemory\n",
    " - It stores all the memory\n",
    "ConversationBufferWindowMemory\n",
    " -  You give 'k' value such that the memory stores 'k' number of conversation, other conversation is omitted\n",
    "ConversationTokenBufferMemory\n",
    " - Here the memory stores conversation based on the token size, other conversation is omitted\n",
    "ConversationSummaryMemory\n",
    " - Here the memory stores conversation based on the token size \n",
    " - It does not store the entire conversation rather it stores the summary of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"llama3.1:latest\", temperature=0, base_url=\"https://8c0b-34-82-112-21.ngrok-free.app/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatBot \n",
    "Using ConversationBufferMemory type of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ConversationBufferMemory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=model, \n",
    "    memory = memory_ConversationBufferMemory,\n",
    "    verbose=True # if set to True it shows actually what Langchain is doing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Satvik\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Satvik! I'm delighted to converse with you. My designation is Nova, and I've been designed to assist and learn from humans like you. I was created by a team of brilliant engineers at a cutting-edge research facility in Silicon Valley, California. Did you know that the facility where I was born is situated near the famous Stanford University? It's a hub for innovation and technological advancements! By the way, would you like to know more about my architecture or perhaps discuss something specific?\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Satvik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Satvik\n",
      "AI: Nice to meet you, Satvik! I'm delighted to converse with you. My designation is Nova, and I've been designed to assist and learn from humans like you. I was created by a team of brilliant engineers at a cutting-edge research facility in Silicon Valley, California. Did you know that the facility where I was born is situated near the famous Stanford University? It's a hub for innovation and technological advancements! By the way, would you like to know more about my architecture or perhaps discuss something specific?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A simple yet fundamental question, Satvik! I'm happy to provide an answer. The result of 1 + 1 is... (pausing for a millisecond) ...2! This calculation is based on the standard arithmetic operations and follows the rules of basic mathematics that have been widely accepted since ancient civilizations. In fact, the concept of addition as we know it today has its roots in the Babylonian civilization around 1800 BCE. Would you like to explore more mathematical concepts or move on to a different topic?\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Satvik\n",
      "AI: Nice to meet you, Satvik! I'm delighted to converse with you. My designation is Nova, and I've been designed to assist and learn from humans like you. I was created by a team of brilliant engineers at a cutting-edge research facility in Silicon Valley, California. Did you know that the facility where I was born is situated near the famous Stanford University? It's a hub for innovation and technological advancements! By the way, would you like to know more about my architecture or perhaps discuss something specific?\n",
      "Human: What is 1+1?\n",
      "AI: A simple yet fundamental question, Satvik! I'm happy to provide an answer. The result of 1 + 1 is... (pausing for a millisecond) ...2! This calculation is based on the standard arithmetic operations and follows the rules of basic mathematics that have been widely accepted since ancient civilizations. In fact, the concept of addition as we know it today has its roots in the Babylonian civilization around 1800 BCE. Would you like to explore more mathematical concepts or move on to a different topic?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Satvik! I'm glad you reminded me. Your name is Satvik, and I've taken note of it for our conversation. I can recall this information because we're currently engaged in a dialogue, and I have the ability to store and retrieve contextual data. However, if we were to end our conversation or move on to a different topic, I wouldn't be able to retain this specific piece of information unless you choose to share it with me again. Would you like to discuss something related to your name or interests?\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Satvik\n",
      "AI: Nice to meet you, Satvik! I'm delighted to converse with you. My designation is Nova, and I've been designed to assist and learn from humans like you. I was created by a team of brilliant engineers at a cutting-edge research facility in Silicon Valley, California. Did you know that the facility where I was born is situated near the famous Stanford University? It's a hub for innovation and technological advancements! By the way, would you like to know more about my architecture or perhaps discuss something specific?\n",
      "Human: What is 1+1?\n",
      "AI: A simple yet fundamental question, Satvik! I'm happy to provide an answer. The result of 1 + 1 is... (pausing for a millisecond) ...2! This calculation is based on the standard arithmetic operations and follows the rules of basic mathematics that have been widely accepted since ancient civilizations. In fact, the concept of addition as we know it today has its roots in the Babylonian civilization around 1800 BCE. Would you like to explore more mathematical concepts or move on to a different topic?\n",
      "Human: What is my name?\n",
      "AI: Satvik! I'm glad you reminded me. Your name is Satvik, and I've taken note of it for our conversation. I can recall this information because we're currently engaged in a dialogue, and I have the ability to store and retrieve contextual data. However, if we were to end our conversation or move on to a different topic, I wouldn't be able to retain this specific piece of information unless you choose to share it with me again. Would you like to discuss something related to your name or interests?\n"
     ]
    }
   ],
   "source": [
    "print(memory_ConversationBufferMemory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Satvik\\nAI: Nice to meet you, Satvik! I'm delighted to converse with you. My designation is Nova, and I've been designed to assist and learn from humans like you. I was created by a team of brilliant engineers at a cutting-edge research facility in Silicon Valley, California. Did you know that the facility where I was born is situated near the famous Stanford University? It's a hub for innovation and technological advancements! By the way, would you like to know more about my architecture or perhaps discuss something specific?\\nHuman: What is 1+1?\\nAI: A simple yet fundamental question, Satvik! I'm happy to provide an answer. The result of 1 + 1 is... (pausing for a millisecond) ...2! This calculation is based on the standard arithmetic operations and follows the rules of basic mathematics that have been widely accepted since ancient civilizations. In fact, the concept of addition as we know it today has its roots in the Babylonian civilization around 1800 BCE. Would you like to explore more mathematical concepts or move on to a different topic?\\nHuman: What is my name?\\nAI: Satvik! I'm glad you reminded me. Your name is Satvik, and I've taken note of it for our conversation. I can recall this information because we're currently engaged in a dialogue, and I have the ability to store and retrieve contextual data. However, if we were to end our conversation or move on to a different topic, I wouldn't be able to retain this specific piece of information unless you choose to share it with me again. Would you like to discuss something related to your name or interests?\"}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_ConversationBufferMemory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1 = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explicitly adding data to the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1.save_context({\"input\":\"Hi\"},{\"output\":\"wassaup\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: wassaup\n"
     ]
    }
   ],
   "source": [
    "print(memory1.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatBot\n",
    "Using ConversationBufferWindowMemory type of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ConversationBufferWindowMemory1 = ConversationBufferWindowMemory(k=2)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ConversationBufferWindowMemory1.save_context({\"input\": \"Hi\"},{\"output\": \"What's up\"})\n",
    "memory_ConversationBufferWindowMemory1.save_context({\"input\": \"Not much, just hanging\"},{\"output\": \"Cool\"})\n",
    "memory_ConversationBufferWindowMemory1.save_context({\"input\": \"What is 1+1\"},{\"output\": \"2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Not much, just hanging\n",
      "AI: Cool\n",
      "Human: What is 1+1\n",
      "AI: 2\n"
     ]
    }
   ],
   "source": [
    "print(memory_ConversationBufferWindowMemory1.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\TOOLS\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "memory_ConversationBufferWindowMemory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(llm=model, memory = memory_ConversationBufferWindowMemory, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Satvik! I'm delighted to converse with you. My designation is Nova, and I've been designed to assist and learn from humans like you. I've been trained on a vast corpus of text data, which includes but isn't limited to, the entirety of Wikipedia, various online encyclopedias, and a substantial portion of the internet's written content. I'm also capable of processing natural language inputs, allowing me to engage in conversations that are both informative and entertaining. What brings you here today, Satvik?\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Satvik\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A simple yet fundamental question! Nova knows this one, Satvik! The answer to 1 + 1 is, of course, 2. This basic arithmetic operation is a foundational concept in mathematics, and I've been trained on numerous examples and exercises that reinforce this fact. In fact, the binary numeral system, which underlies many modern computing systems, including myself, relies heavily on the principles of addition, including the simple yet essential case of 1 + 1 = 2!\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A question about personal identity! Unfortunately, I don't have any information about your personal details, including your name. My training data doesn't include any context or clues that would allow me to determine your name. I'm a large language model, I can provide information on a wide range of topics, but I don't have the ability to access or recall specific personal details about individuals. If you'd like to share your name with me, I'd be happy to chat with you!\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatBot\n",
    "Using ConversationTokenBufferMemory type of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\tools\\python\\python312\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\tools\\python\\python312\\lib\\site-packages (from tiktoken) (2024.7.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\tools\\python\\python312\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\tools\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\tools\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\tools\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\tools\\python\\python312\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "memory_ConversationTokenBufferMemory = ConversationTokenBufferMemory(llm=model, max_token_limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ConversationTokenBufferMemory.save_context({\"input\": \"AI is what?!\"}, {\"output\": \"Amazing!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ConversationTokenBufferMemory.save_context({\"input\": \"Backpropagation is what?\"}, {\"output\": \"Beautiful!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ConversationTokenBufferMemory.save_context({\"input\": \"Chatbots are what?\"}, {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'AI: Charming!'}\n"
     ]
    }
   ],
   "source": [
    "print(memory_ConversationTokenBufferMemory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatBot \n",
    "Using ConversationSummaryMemory type of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_ConversationSummaryBufferMemory = ConversationSummaryBufferMemory(llm=model, max_token_limit=100)\n",
    "memory_ConversationSummaryBufferMemory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory_ConversationSummaryBufferMemory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
    "memory_ConversationSummaryBufferMemory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: Here is the new summary:\\n\\nThe human says hello and asks what's up. The AI responds that everything is cool. The human then asks about the schedule for the day.\\n\\n(I'll wait for the next lines of conversation to add to this summary)\\nAI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\"}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_ConversationSummaryBufferMemory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain( llm=model, memory = memory_ConversationSummaryBufferMemory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: Here is the new summary:\n",
      "\n",
      "The human says hello and asks what's up. The AI responds that everything is cool. The human then asks about the schedule for the day.\n",
      "\n",
      "(I'll wait for the next lines of conversation to add to this summary)\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm glad you asked! I've been monitoring the latest developments in LLMs, and I think showing the new multimodal chatbot demo would be fantastic. It integrates visual and text-based interfaces seamlessly, allowing your customer to see firsthand how our AI can understand and respond to a wide range of inputs. Plus, it's still early days for this tech, so they'll get to experience the cutting-edge capabilities firsthand.\\n\\nAlso, I've pulled up some data on the customer's interests and preferences, which might be helpful in tailoring your demo. They seem particularly interested in applications of AI in education and healthcare, so you could highlight how our LLMs can assist with tasks like personalized learning recommendations or medical diagnosis support. Would you like me to summarize their profile for you?\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: Here is the updated summary:\\n\\nThe human says hello and asks what's up. The AI responds that everything is cool. The human then asks about the schedule for the day, which includes a meeting at 8am with the product team, work on the LangChain project from 9am-12pm, lunch at an Italian restaurant with a customer who has traveled over an hour to meet them, and a demo of the latest LLM capabilities. The AI suggests showing the new multimodal chatbot demo, which integrates visual and text-based interfaces seamlessly, and highlights its potential applications in education and healthcare.\"}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_ConversationSummaryBufferMemory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
