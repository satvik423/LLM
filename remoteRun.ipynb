{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "model = Ollama(model=\"llama3.1:latest\", temperature=0, base_url=\"https://8c0b-34-82-112-21.ngrok-free.app/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My character limit is 2048 characters, which is equivalent to approximately **322-333 words**, depending on the formatting and spacing.\n",
      "\n",
      "This means I can provide a detailed response with multiple paragraphs, examples, or explanations within this word count. However, if you need more information or context, feel free to ask me to elaborate or provide additional details!\n",
      "\n",
      "To give you a better idea, here are some rough estimates of what you can expect from my responses based on the character limit:\n",
      "\n",
      "* Short answers: 1-100 characters (e.g., definitions, simple facts)\n",
      "* Brief explanations: 101-500 characters (e.g., short summaries, basic concepts)\n",
      "* Standard responses: 501-2048 characters (e.g., detailed explanations, examples, and context)\n",
      "\n",
      "Keep in mind that these are rough estimates, and the actual length of my response may vary depending on the complexity of the topic or question.\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"Whats your context lenght? Whats the maximum lenght [words] of reply can you give?  \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a great project! I'd be happy to help.\n",
      "\n",
      "Yes, I can be a suitable model for this RAG (Retrieve, Answer, Generate) pipeline. Here's why:\n",
      "\n",
      "1. **Knowledge base**: My training data includes a vast amount of text from various sources, including books, articles, and websites. This knowledge base can serve as the foundation for storing subject notes.\n",
      "2. **Question answering**: I'm designed to answer questions based on my training data. With the right configuration, I can be fine-tuned to provide accurate answers related to specific subjects.\n",
      "3. **Text generation**: When a student asks a question, I can generate an answer based on the relevant information from the notes.\n",
      "\n",
      "To achieve this, follow these steps:\n",
      "\n",
      "**Step 1: Data Preparation**\n",
      "\n",
      "* Collect subject notes from lectures in a structured format (e.g., Markdown or JSON).\n",
      "* Ensure the notes are well-organized and easily searchable.\n",
      "* Consider using a database to store the notes for efficient retrieval.\n",
      "\n",
      "**Step 2: Model Configuration**\n",
      "\n",
      "* Integrate my API into your chatbot platform.\n",
      "* Configure me to use the subject notes as the primary knowledge base.\n",
      "* Fine-tune my parameters to optimize performance for question answering related to the specific subjects.\n",
      "\n",
      "**Step 3: Question Answering Logic**\n",
      "\n",
      "* Implement a logic that allows students to ask questions related to specific subjects (e.g., \"What is the capital of France?\" or \"What are the key concepts in Calculus?\").\n",
      "* Use natural language processing (NLP) techniques to extract relevant information from the notes and provide accurate answers.\n",
      "\n",
      "**Step 4: Text Generation**\n",
      "\n",
      "* When a student asks a question, use my text generation capabilities to create an answer based on the relevant information from the notes.\n",
      "* Consider using a template or a specific format for generating answers to ensure consistency and clarity.\n",
      "\n",
      "Some other models that could be suitable for this task are:\n",
      "\n",
      "1. **Transformers**: A family of models that excel in natural language processing tasks, such as question answering and text generation.\n",
      "2. **BERT** (Bidirectional Encoder Representations from Transformers): A popular model for NLP tasks, including question answering and text classification.\n",
      "3. **T5** (Text-to-Text Transfer Transformer): A versatile model designed for a wide range of NLP tasks, including text generation and question answering.\n",
      "\n",
      "Keep in mind that these models might require more computational resources and expertise to implement compared to my simple API-based approach.\n",
      "\n",
      "Which model do you think would be the best fit for your project? Or would you like me to help with implementing the steps outlined above?\n"
     ]
    }
   ],
   "source": [
    "print(model.invoke(\"I am building a chatbot for the college using you, basically its a RAG pipeline such that the lectures add their subject notes and students can ask questions related to it, so you will answer the students questions based on the notes that the lectures upload. Are you a sutable model for that? if yes, tell me the steps to achive it and if no suggest me some other model that is capable of it\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
